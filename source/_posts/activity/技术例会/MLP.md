---
abbrlink: 1050790594
title: MLP教学例会活动总结
type: activity
author: 南的AIA
date: 2024-11-3
description: 神经网络--多层感知机(MLP)模型
---


![](/images/MLP/1.jpg)

### 引入

本次教学例会的主讲人是岳逸帆。他提到：今年的诺贝尔奖颁给了人工智能领域的 Hinton ，其主要研究成果聚焦于 ANN（人工神经网络，Artificial Neural Network），即MLP（多层感知机，Multilayer Perceptron）。今天我们可以通过简单的讲解自己“手搓”一个 MLP 。 MLP 虽看似复杂，本质却只是一种数学方法。我们通过数学和编程工具，可以更好地理解并构建 MLP。 人工神经网络，听上去就像计算机也有生物体的神经网络一样。事实上，我们可以将人工神经网络比作生物神经网络——它们都是通过“神经元”传递和处理信息的系统。然而，ANN 的工作机制本质上是以数学为基础的，通过激活函数，神经元才能有条件地输出大量信号，而激活函数本身是一种数学操作，这在生物的神经网络有着本质的区别。


### 数学解析：激活函数、神经元与隐藏层

#### 激活函数的类型

激活函数的类型在本次例会中，我们介绍了以下几种基本的激活函数，包括阶跃函数；

Sigmoid函数：$S(x)= \frac{1}{1+e^{-x}}$

```
def sigmoid(x):    
    return 1/(1+np.exp(-x))
```

ReLU函数（Rectified Linear Unit）：$f(x)=max(x,0)$

```
def relu(x):    
    return np.maximum(x,0)
```

这些函数将输入值转化为特定范围的输出，使得网络能够进行更复杂的非线性映射。

#### 神经元的工作机制

MLP中每个神经元的输出公式为 $f(\sum{w_ix_i}+b) $，其中 $w_i$ 代表权重， $x_i$ 是输入， $b$ 是偏置，$f$ 是激活函数。尽管公式看似复杂，但通过绘制示意图，便能清晰地理解加和符号 $\sum{}$ 、权重 $w$ 和偏置 $b$ 的含义。激活函数 $f$ 的存在，赋予了模型进行复杂映射的能力；若缺少，则无论叠加多少层，模型的表达能力都有限，甚至无法正确地解决简单的 XOR 问题。

#### 隐藏层的必要性

我们通过与或非门和异或门的实例讨论了隐藏层的作用，进一步讲解了线性可分与非线性可分的概念。隐藏层使得神经网络能够近似任意函数，这就是“万有逼近定理”所揭示的内容。深度和宽度的不同组合可以影响网络的表现，使得网络具备更高的表达能力。


###  使用NumPy实现基础运算

为了让大家掌握MLP的实现，我们还进行了半个小时的NumPy教学，涵盖了向量和矩阵的创建与操作。

#### 创建向量/矩阵及其形状

在 Python 中使用 NumPy 创建数组非常便捷，例如 array、zeros、linspace 等操作能轻松生成不同形状的向量和矩阵。同时，通过 shape 属性，我们可以查看数组的维度，便于后续的数学运算。

#### 矩阵的运算

矩阵乘法、点乘、逆矩阵、伪逆矩阵以及特征值等操作是构建 MLP 的基础。我们在代码实践中，通过 @ 符号实现矩阵乘法，用 * 实现点乘。通过这部分的学习，大家具备了使用 NumPy 实现基本 MLP 运算的能力。

### 代码实战

随后的一个小时，同学们在助教的悉心帮助下，全身心投入到手搓代码的任务当中。助教们穿梭于同学们之间，针对大家在编写代码过程中遇到的各类问题，如语法错误、逻辑不清、算法实现疑惑等，及时给予耐心细致的解答与指导。同学们则依据之前所学的数学知识、numpy 相关操作等内容，一步一个脚印地将代码从无到有地“搓”出来，通过不断调试与完善，逐步实现了与 MLP 相关的各项功能代码的编写，切实将理论知识转化为实际的代码成果，进一步加深了对 MLP 的理解与掌握。

本次活动从理论到实践，层层递进，使得大家对多层感知机（MLP）有了一个全方位的理解。这不仅加深了大家对人工神经网络的认识，还培养了实际编程能力。

![](/images/MLP/2.png)

[阅读原文](https://mp.weixin.qq.com/s/O8_7OchrD2uljW3vhhrqgQ)



> 文编|李炎培 岳逸帆  
美编|张馨月